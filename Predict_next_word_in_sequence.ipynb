{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFWbEb6uGbN-"
      },
      "source": [
        "# Predicting the next word\n",
        "\n",
        "Create a model that will predict the next word in a text sequence, here it is implemented and trained using a corpus of Shakespeare's sonnets, while also creating some helper functions to pre-process the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BOwsuGQQY9OL",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.utils as ku \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTxqlHqKHzhr"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Here the [Shakespeare Sonnets Dataset](https://www.opensourceshakespeare.org/views/sonnets/sonnet_view.php?range=viewrange&sonnetrange1=1&sonnetrange2=154), which contains more than 2000 lines of text extracted from Shakespeare's sonnets is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WZ4qOUzujMP6",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c1da4a-d9e2-4478-bb22-7d8faf68a1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n",
            "To: /content/sonnets.txt\n",
            "100% 93.6k/93.6k [00:00<00:00, 81.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# sonnets.txt\n",
        "!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Pfd-nYKij5yY",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbf9372-f8e4-4123-9f87-adf65c4953a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2159 lines of sonnets\n",
            "\n",
            "The first 5 lines look like this:\n",
            "\n",
            "from fairest creatures we desire increase,\n",
            "that thereby beauty's rose might never die,\n",
            "but as the riper should by time decease,\n",
            "his tender heir might bear his memory:\n",
            "but thou, contracted to thine own bright eyes,\n"
          ]
        }
      ],
      "source": [
        "# Define path for file with sonnets\n",
        "SONNETS_FILE = './sonnets.txt'\n",
        "\n",
        "# Read the data\n",
        "with open('./sonnets.txt') as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Convert to lower case and save as a list\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "print(f\"There are {len(corpus)} lines of sonnets\\n\")\n",
        "print(f\"The first 5 lines look like this:\\n\")\n",
        "for i in range(5):\n",
        "  print(corpus[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imB15zrSNhA1"
      },
      "source": [
        "## Tokenizing the text\n",
        "\n",
        "Now fit the Tokenizer to the corpus and save the total number of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AAhM_qAZk0o5",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tqhPxdeXlfjh",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22cd23d3-94d8-4706-f518-4fa53fbad14a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from fairest creatures we desire increase,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "corpus[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kpTy8WmIQ57P",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2993e25-c0ae-4f77-e8c3-118e9d7de767"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[34, 417, 877, 166, 213, 517]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([corpus[0]])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oqy9KjXRJ9A"
      },
      "source": [
        "## Generating n_grams\n",
        "\n",
        "Now complete the `n_gram_seqs` function below. This function receives the fitted tokenizer and the corpus (which is a list of strings) and should return a list containing the `n_gram` sequences for each line in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iy4baJMDl6kj",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: n_gram_seqs\n",
        "def n_gram_seqs(corpus, tokenizer):\n",
        "    \"\"\"\n",
        "    Generates a list of n-gram sequences\n",
        "    \n",
        "    Args:\n",
        "        corpus (list of string): lines of texts to generate n-grams for\n",
        "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
        "    \n",
        "    Returns:\n",
        "        input_sequences (list of int): the n-gram sequences for each line in the corpus\n",
        "    \"\"\"\n",
        "    input_sequences = []\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    for line in corpus:\n",
        "      token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "      for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DlKqW2pfM7G3",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "943643c6-5914-48a1-ef80-db3342c7e051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for first example look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417],\n",
              " [34, 417, 877],\n",
              " [34, 417, 877, 166],\n",
              " [34, 417, 877, 166, 213],\n",
              " [34, 417, 877, 166, 213, 517]]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Testing function with one example\n",
        "first_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for first example look like this:\\n\")\n",
        "first_example_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wtPpCcBjNc4c",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc731a1-1c51-45aa-8782-e6bf1f08c8f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for next 3 examples look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 878],\n",
              " [8, 878, 134],\n",
              " [8, 878, 134, 351],\n",
              " [8, 878, 134, 351, 102],\n",
              " [8, 878, 134, 351, 102, 156],\n",
              " [8, 878, 134, 351, 102, 156, 199],\n",
              " [16, 22],\n",
              " [16, 22, 2],\n",
              " [16, 22, 2, 879],\n",
              " [16, 22, 2, 879, 61],\n",
              " [16, 22, 2, 879, 61, 30],\n",
              " [16, 22, 2, 879, 61, 30, 48],\n",
              " [16, 22, 2, 879, 61, 30, 48, 634],\n",
              " [25, 311],\n",
              " [25, 311, 635],\n",
              " [25, 311, 635, 102],\n",
              " [25, 311, 635, 102, 200],\n",
              " [25, 311, 635, 102, 200, 25],\n",
              " [25, 311, 635, 102, 200, 25, 278]]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Testing function with a bigger corpus\n",
        "next_3_examples_sequence = n_gram_seqs(corpus[1:4], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for next 3 examples look like this:\\n\")\n",
        "next_3_examples_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx3V_RjFWQSu"
      },
      "source": [
        "Apply the `n_gram_seqs` transformation to the whole corpus and save the maximum sequence length to use it later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "laMwiRUpmuSd",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1051fbf8-a77c-452a-bef4-81b658dda390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_grams of input_sequences have length: 15462\n",
            "maximum length of sequences is: 11\n"
          ]
        }
      ],
      "source": [
        "# Apply the n_gram_seqs transformation to the whole corpus\n",
        "input_sequences = n_gram_seqs(corpus, tokenizer)\n",
        "\n",
        "# Save max length \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "print(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\n",
        "print(f\"maximum length of sequences is: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHY7HroqWq12"
      },
      "source": [
        "## Add padding to the sequences\n",
        "\n",
        "Now code the `pad_seqs` function which will pad any given sequences to the desired maximum length. Notice that this function receives a list of sequences and should return a numpy array with the padded sequences: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "cellView": "code",
        "id": "WW1-qAZaWOhC",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: pad_seqs\n",
        "def pad_seqs(input_sequences, maxlen):\n",
        "    \"\"\"\n",
        "    Pads tokenized sequences to the same length\n",
        "    \n",
        "    Args:\n",
        "        input_sequences (list of int): tokenized sequences to pad\n",
        "        maxlen (int): maximum length of the token sequences\n",
        "    \n",
        "    Returns:\n",
        "        padded_sequences (array of int): tokenized sequences padded to the same length\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    padded_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "    \n",
        "    return padded_sequences\n",
        "    ### END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IqVQ0pb3YHLr",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e00623-924e-4a95-af67-814cef7f21f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,  34, 417],\n",
              "       [  0,   0,   0,  34, 417, 877],\n",
              "       [  0,   0,  34, 417, 877, 166],\n",
              "       [  0,  34, 417, 877, 166, 213],\n",
              "       [ 34, 417, 877, 166, 213, 517]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Test your function with the n_grams_seq of the first example\n",
        "first_padded_seq = pad_seqs(first_example_sequence, len(first_example_sequence))\n",
        "first_padded_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "j56_UCOBYzZt",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90880c44-f868-4e3b-801d-9979604e0f25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   8, 878],\n",
              "       [  0,   0,   0,   0,   0,   8, 878, 134],\n",
              "       [  0,   0,   0,   0,   8, 878, 134, 351],\n",
              "       [  0,   0,   0,   8, 878, 134, 351, 102],\n",
              "       [  0,   0,   8, 878, 134, 351, 102, 156],\n",
              "       [  0,   8, 878, 134, 351, 102, 156, 199],\n",
              "       [  0,   0,   0,   0,   0,   0,  16,  22],\n",
              "       [  0,   0,   0,   0,   0,  16,  22,   2],\n",
              "       [  0,   0,   0,   0,  16,  22,   2, 879],\n",
              "       [  0,   0,   0,  16,  22,   2, 879,  61],\n",
              "       [  0,   0,  16,  22,   2, 879,  61,  30],\n",
              "       [  0,  16,  22,   2, 879,  61,  30,  48],\n",
              "       [ 16,  22,   2, 879,  61,  30,  48, 634],\n",
              "       [  0,   0,   0,   0,   0,   0,  25, 311],\n",
              "       [  0,   0,   0,   0,   0,  25, 311, 635],\n",
              "       [  0,   0,   0,   0,  25, 311, 635, 102],\n",
              "       [  0,   0,   0,  25, 311, 635, 102, 200],\n",
              "       [  0,   0,  25, 311, 635, 102, 200,  25],\n",
              "       [  0,  25, 311, 635, 102, 200,  25, 278]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Test your function with the n_grams_seq of the next 3 examples\n",
        "next_3_padded_seq = pad_seqs(next_3_examples_sequence, max([len(s) for s in next_3_examples_sequence]))\n",
        "next_3_padded_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rgK-Q_micEYA",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa0726f2-86e2-44eb-9081-c6744f5f3d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padded corpus has shape: (15462, 11)\n"
          ]
        }
      ],
      "source": [
        "# Pad the whole corpus\n",
        "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
        "\n",
        "print(f\"padded corpus has shape: {input_sequences.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbOidyPrXxf7"
      },
      "source": [
        "## Split the data into features and labels\n",
        "\n",
        "Before feeding the data into the neural network you should split it into features and labels. In this case the features will be the padded n_gram sequences with the last word removed from them and the labels will be the removed word.\n",
        "\n",
        "`features_and_labels` function : This function expects the padded n_gram sequences as input and should return a tuple containing the features and the one hot encoded labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "cellView": "code",
        "id": "9WGGbYdnZdmJ",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: features_and_labels\n",
        "def features_and_labels(input_sequences, total_words):\n",
        "    \"\"\"\n",
        "    Generates features and labels from n-grams\n",
        "    \n",
        "    Args:\n",
        "        input_sequences (list of int): sequences to split features and labels from\n",
        "        total_words (int): vocabulary size\n",
        "    \n",
        "    Returns:\n",
        "        features, one_hot_labels (array of int, array of int): arrays of features and one-hot encoded labels\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "    features = input_sequences[:,:-1]\n",
        "    labels = input_sequences[:,-1]\n",
        "    one_hot_labels = ku.to_categorical(labels, num_classes=total_words)\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return features, one_hot_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "23DolaBRaIAZ",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c06141e-c1a2-4829-f55c-513bab8a866c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels have shape: (5, 3211)\n",
            "\n",
            "features look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,  34],\n",
              "       [  0,   0,   0,  34, 417],\n",
              "       [  0,   0,  34, 417, 877],\n",
              "       [  0,  34, 417, 877, 166],\n",
              "       [ 34, 417, 877, 166, 213]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# Test your function with the padded n_grams_seq of the first example\n",
        "first_features, first_labels = features_and_labels(first_padded_seq, total_words)\n",
        "\n",
        "print(f\"labels have shape: {first_labels.shape}\")\n",
        "print(\"\\nfeatures look like this:\\n\")\n",
        "first_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "GRTuLEt3bRKa",
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0dee92-2b60-4bd0-d826-1193bceae72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features have shape: (15462, 10)\n",
            "labels have shape: (15462, 3211)\n"
          ]
        }
      ],
      "source": [
        "# Split the whole corpus\n",
        "features, labels = features_and_labels(input_sequences, total_words)\n",
        "\n",
        "print(f\"features have shape: {features.shape}\")\n",
        "print(f\"labels have shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltxaOCE_aU6J"
      },
      "source": [
        "## Create the model\n",
        "\n",
        "Model architecture should be capable of achieving an accuracy of at least 80%.\n",
        "\n",
        "- An appropriate `output_dim` for the first layer (Embedding) is 100, this is already provided for you.\n",
        "- A Bidirectional LSTM is helpful for this particular problem.\n",
        "- The last layer should have the same number of units as the total number of words in the corpus and a softmax activation function.\n",
        "- This problem can be solved with only two layers (excluding the Embedding) so try out small architectures first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "cellView": "code",
        "id": "XrE6kpJFfvRY",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: create_model\n",
        "def create_model(total_words, max_sequence_len):\n",
        "    \"\"\"\n",
        "    Creates a text generator model\n",
        "    \n",
        "    Args:\n",
        "        total_words (int): size of the vocabulary for the Embedding layer input\n",
        "        max_sequence_len (int): length of the input sequences\n",
        "    \n",
        "    Returns:\n",
        "        model (tf.keras Model): the text generator model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    ### START CODE HERE\n",
        "    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1)) # Your Embedding Layer)\n",
        "    model.add(Bidirectional(LSTM(100, return_sequences = True)))            # An LSTM Layer)\n",
        "    model.add(Dropout(0.2))                                                # A dropout layer)\n",
        "    model.add(Bidirectional(LSTM(100)))                                     # Another LSTM Layer)\n",
        "    model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))      # A Dense Layer including regularizers)\n",
        "    model.add(Dense(total_words, activation='softmax'))                    # A Dense Layer)\n",
        "    # Pick an optimizer\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])# Pick a loss function and an optimizer)\n",
        "\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "0IpX_Gu_gISk",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67aad63a-b8b4-4a35-8e6b-80fb52436749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/140\n",
            "484/484 [==============================] - 16s 12ms/step - loss: 6.9715 - accuracy: 0.0205\n",
            "Epoch 2/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 6.5179 - accuracy: 0.0216\n",
            "Epoch 3/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 6.4133 - accuracy: 0.0229\n",
            "Epoch 4/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 6.2877 - accuracy: 0.0287\n",
            "Epoch 5/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 6.1873 - accuracy: 0.0350\n",
            "Epoch 6/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 6.0947 - accuracy: 0.0384\n",
            "Epoch 7/140\n",
            "484/484 [==============================] - 7s 15ms/step - loss: 5.9951 - accuracy: 0.0407\n",
            "Epoch 8/140\n",
            "484/484 [==============================] - 7s 14ms/step - loss: 5.8795 - accuracy: 0.0448\n",
            "Epoch 9/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.7521 - accuracy: 0.0526\n",
            "Epoch 10/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.6341 - accuracy: 0.0616\n",
            "Epoch 11/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.5128 - accuracy: 0.0693\n",
            "Epoch 12/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.3958 - accuracy: 0.0765\n",
            "Epoch 13/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.2813 - accuracy: 0.0847\n",
            "Epoch 14/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.1663 - accuracy: 0.0923\n",
            "Epoch 15/140\n",
            "484/484 [==============================] - 6s 13ms/step - loss: 5.0487 - accuracy: 0.1034\n",
            "Epoch 16/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.9404 - accuracy: 0.1096\n",
            "Epoch 17/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.8255 - accuracy: 0.1190\n",
            "Epoch 18/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.7111 - accuracy: 0.1309\n",
            "Epoch 19/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.6091 - accuracy: 0.1379\n",
            "Epoch 20/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.4905 - accuracy: 0.1484\n",
            "Epoch 21/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.3806 - accuracy: 0.1612\n",
            "Epoch 22/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.2719 - accuracy: 0.1705\n",
            "Epoch 23/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.1553 - accuracy: 0.1846\n",
            "Epoch 24/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.0535 - accuracy: 0.1953\n",
            "Epoch 25/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.9528 - accuracy: 0.2106\n",
            "Epoch 26/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.8508 - accuracy: 0.2273\n",
            "Epoch 27/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.7549 - accuracy: 0.2434\n",
            "Epoch 28/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.6689 - accuracy: 0.2573\n",
            "Epoch 29/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.5631 - accuracy: 0.2793\n",
            "Epoch 30/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.4808 - accuracy: 0.2947\n",
            "Epoch 31/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.3953 - accuracy: 0.3137\n",
            "Epoch 32/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.3159 - accuracy: 0.3331\n",
            "Epoch 33/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.2230 - accuracy: 0.3514\n",
            "Epoch 34/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.1644 - accuracy: 0.3626\n",
            "Epoch 35/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.0728 - accuracy: 0.3845\n",
            "Epoch 36/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.0070 - accuracy: 0.3981\n",
            "Epoch 37/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.9293 - accuracy: 0.4186\n",
            "Epoch 38/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.8648 - accuracy: 0.4322\n",
            "Epoch 39/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.8058 - accuracy: 0.4449\n",
            "Epoch 40/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.7297 - accuracy: 0.4627\n",
            "Epoch 41/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.6703 - accuracy: 0.4770\n",
            "Epoch 42/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.6010 - accuracy: 0.4937\n",
            "Epoch 43/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.5591 - accuracy: 0.5028\n",
            "Epoch 44/140\n",
            "484/484 [==============================] - 6s 13ms/step - loss: 2.4905 - accuracy: 0.5197\n",
            "Epoch 45/140\n",
            "484/484 [==============================] - 7s 14ms/step - loss: 2.4360 - accuracy: 0.5329\n",
            "Epoch 46/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.3939 - accuracy: 0.5410\n",
            "Epoch 47/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.3312 - accuracy: 0.5550\n",
            "Epoch 48/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.2826 - accuracy: 0.5675\n",
            "Epoch 49/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.2475 - accuracy: 0.5797\n",
            "Epoch 50/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.2057 - accuracy: 0.5864\n",
            "Epoch 51/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.1678 - accuracy: 0.5933\n",
            "Epoch 52/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.1133 - accuracy: 0.6076\n",
            "Epoch 53/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.0638 - accuracy: 0.6193\n",
            "Epoch 54/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.0218 - accuracy: 0.6281\n",
            "Epoch 55/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.9805 - accuracy: 0.6418\n",
            "Epoch 56/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.9626 - accuracy: 0.6411\n",
            "Epoch 57/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.9197 - accuracy: 0.6541\n",
            "Epoch 58/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.8764 - accuracy: 0.6575\n",
            "Epoch 59/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.8445 - accuracy: 0.6654\n",
            "Epoch 60/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.8185 - accuracy: 0.6733\n",
            "Epoch 61/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.7908 - accuracy: 0.6824\n",
            "Epoch 62/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.7538 - accuracy: 0.6867\n",
            "Epoch 63/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.7328 - accuracy: 0.6928\n",
            "Epoch 64/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.7086 - accuracy: 0.6987\n",
            "Epoch 65/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.6780 - accuracy: 0.7017\n",
            "Epoch 66/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.6431 - accuracy: 0.7123\n",
            "Epoch 67/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.6234 - accuracy: 0.7094\n",
            "Epoch 68/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.5912 - accuracy: 0.7233\n",
            "Epoch 69/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.5820 - accuracy: 0.7236\n",
            "Epoch 70/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.5512 - accuracy: 0.7313\n",
            "Epoch 71/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.5337 - accuracy: 0.7311\n",
            "Epoch 72/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.5042 - accuracy: 0.7394\n",
            "Epoch 73/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.4970 - accuracy: 0.7388\n",
            "Epoch 74/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.4771 - accuracy: 0.7415\n",
            "Epoch 75/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.4514 - accuracy: 0.7462\n",
            "Epoch 76/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.4378 - accuracy: 0.7502\n",
            "Epoch 77/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.4311 - accuracy: 0.7509\n",
            "Epoch 78/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.3919 - accuracy: 0.7624\n",
            "Epoch 79/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.3787 - accuracy: 0.7629\n",
            "Epoch 80/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.3789 - accuracy: 0.7634\n",
            "Epoch 81/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.3649 - accuracy: 0.7617\n",
            "Epoch 82/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.3369 - accuracy: 0.7716\n",
            "Epoch 83/140\n",
            "484/484 [==============================] - 8s 16ms/step - loss: 1.3379 - accuracy: 0.7678\n",
            "Epoch 84/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.3237 - accuracy: 0.7694\n",
            "Epoch 85/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.3024 - accuracy: 0.7754\n",
            "Epoch 86/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.2796 - accuracy: 0.7792\n",
            "Epoch 87/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.2829 - accuracy: 0.7767\n",
            "Epoch 88/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.2514 - accuracy: 0.7832\n",
            "Epoch 89/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.2502 - accuracy: 0.7829\n",
            "Epoch 90/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.2415 - accuracy: 0.7857\n",
            "Epoch 91/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.2396 - accuracy: 0.7849\n",
            "Epoch 92/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.2275 - accuracy: 0.7856\n",
            "Epoch 93/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.2031 - accuracy: 0.7932\n",
            "Epoch 94/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1929 - accuracy: 0.7966\n",
            "Epoch 95/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1936 - accuracy: 0.7896\n",
            "Epoch 96/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1801 - accuracy: 0.7952\n",
            "Epoch 97/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1736 - accuracy: 0.7975\n",
            "Epoch 98/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1557 - accuracy: 0.7998\n",
            "Epoch 99/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1666 - accuracy: 0.7937\n",
            "Epoch 100/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1435 - accuracy: 0.8022\n",
            "Epoch 101/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1675 - accuracy: 0.7974\n",
            "Epoch 102/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1469 - accuracy: 0.7986\n",
            "Epoch 103/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1214 - accuracy: 0.8029\n",
            "Epoch 104/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1237 - accuracy: 0.8025\n",
            "Epoch 105/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1101 - accuracy: 0.8027\n",
            "Epoch 106/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.1071 - accuracy: 0.8052\n",
            "Epoch 107/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0877 - accuracy: 0.8098\n",
            "Epoch 108/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0862 - accuracy: 0.8060\n",
            "Epoch 109/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0766 - accuracy: 0.8117\n",
            "Epoch 110/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0758 - accuracy: 0.8102\n",
            "Epoch 111/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0744 - accuracy: 0.8123\n",
            "Epoch 112/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0712 - accuracy: 0.8092\n",
            "Epoch 113/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0748 - accuracy: 0.8086\n",
            "Epoch 114/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0509 - accuracy: 0.8143\n",
            "Epoch 115/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0247 - accuracy: 0.8202\n",
            "Epoch 116/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0341 - accuracy: 0.8163\n",
            "Epoch 117/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0307 - accuracy: 0.8180\n",
            "Epoch 118/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0360 - accuracy: 0.8149\n",
            "Epoch 119/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0379 - accuracy: 0.8135\n",
            "Epoch 120/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0294 - accuracy: 0.8143\n",
            "Epoch 121/140\n",
            "484/484 [==============================] - 8s 16ms/step - loss: 1.0249 - accuracy: 0.8152\n",
            "Epoch 122/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0101 - accuracy: 0.8177\n",
            "Epoch 123/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0092 - accuracy: 0.8185\n",
            "Epoch 124/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0003 - accuracy: 0.8203\n",
            "Epoch 125/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0015 - accuracy: 0.8182\n",
            "Epoch 126/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.0032 - accuracy: 0.8165\n",
            "Epoch 127/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9964 - accuracy: 0.8198\n",
            "Epoch 128/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9655 - accuracy: 0.8268\n",
            "Epoch 129/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9679 - accuracy: 0.8242\n",
            "Epoch 130/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9692 - accuracy: 0.8235\n",
            "Epoch 131/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9899 - accuracy: 0.8211\n",
            "Epoch 132/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9738 - accuracy: 0.8240\n",
            "Epoch 133/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9826 - accuracy: 0.8193\n",
            "Epoch 134/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9616 - accuracy: 0.8242\n",
            "Epoch 135/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9809 - accuracy: 0.8178\n",
            "Epoch 136/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9554 - accuracy: 0.8237\n",
            "Epoch 137/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9493 - accuracy: 0.8261\n",
            "Epoch 138/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9482 - accuracy: 0.8267\n",
            "Epoch 139/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9527 - accuracy: 0.8234\n",
            "Epoch 140/140\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 0.9456 - accuracy: 0.8261\n"
          ]
        }
      ],
      "source": [
        "# Get the untrained model\n",
        "model = create_model(total_words, max_sequence_len)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(features, labels, epochs=140, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1fXTEO3GJ282",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "3a621e61-416c-4277-b3a1-dc140582a549"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Take a look at the training curves of your model\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "9QRG73l6qE-c",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "accdd16d-167b-4cec-ee47-09b56ccbeb10"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dab94403-3df6-4b8c-84d2-9bbeb111c785\", \"history.pkl\", 2562)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def download_history():\n",
        "  import pickle\n",
        "  from google.colab import files\n",
        "\n",
        "  with open('history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "  files.download('history.pkl')\n",
        "\n",
        "download_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdsMszk9zBs_"
      },
      "source": [
        "## See model in action\n",
        "\n",
        "Run the cell below to generate the next 100 words of a seed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "6Vc6PHgxa6Hm",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82728f7-e574-4d60-e8e3-f6cdb722140a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help me Obi Wan Kenobi, you're my only hope name they buried of life tell mine eyes common grow on earth nearly hour such summer's gait treason cold holds from men away ' back again shall play to stand cold cold doth date disarm'd disarm'd set dead burn her heart torn seen releasing far mad fix'd so better air in no lips more write to you best best so show thee thine old old rent near slain near wilt shine bright lies free seen weep held weep new bold eye on nought better hate on ' alone so seen another youth and life i shows not they lie to\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\t# Convert the text into sequences\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\t# Pad the sequences\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\t# Get the probabilities of predicting a word\n",
        "\tpredicted = model.predict(token_list, verbose=0)\n",
        "\t# Choose the next word based on the maximum probability\n",
        "\tpredicted = np.argmax(predicted, axis=-1).item()\n",
        "\t# Get the actual word from the word index\n",
        "\toutput_word = tokenizer.index_word[predicted]\n",
        "\t# Append to the current text\n",
        "\tseed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "dlai_version": "1.2.0",
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}